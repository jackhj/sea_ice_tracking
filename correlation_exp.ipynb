{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a cell of text! (Markdown cell)\n",
    "\n",
    "It does nothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, math, matplotlib as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in this box we're going to define our grid and data\n",
    "\n",
    "'''this will be size of grid'''\n",
    "gridSize = 100 \n",
    "testGridSize = 9\n",
    "\n",
    "# produce random data values \n",
    "'''eventually this will be brightness temperature data'''\n",
    "data0 = np.random.rand(np.sqrt(gridSize),np.sqrt(gridSize)) * 10\n",
    "# copy the original data and artificially move the array to the right\n",
    "data1 = data0.copy()\n",
    "#data1 = np.hstack((data1[:,-1].reshape((data1.shape[0], 1)),data1[:,0:-1]))\n",
    "data1[:,0:5] = data0[:,1:6]\n",
    "data1[:,6:] = data0[:,5:-1]\n",
    "\n",
    "# define indices\n",
    "I, J = np.meshgrid(np.arange(0,int(np.sqrt(gridSize))),np.arange(0,int(np.sqrt(gridSize))),indexing='ij')\n",
    "I = I.reshape((I.size, 1))\n",
    "J = J.reshape((J.size, 1))\n",
    "\n",
    "# define locations\n",
    "'''eventually this will be lat and lon data'''\n",
    "Y, X = np.meshgrid(10*np.arange(0,int(np.sqrt(gridSize))),10*np.arange(0,int(np.sqrt(gridSize))),indexing='ij')\n",
    "X = X.reshape((X.size, 1))\n",
    "Y = Y.reshape((Y.size, 1))\n",
    "\n",
    "# keep all location data in one array\n",
    "locationData = np.concatenate((I,J,Y,X),1)\n",
    "\n",
    "# define a don't bother with mask\n",
    "''' this will contain: landmask (and half landmask to avoid footprint overlap), also mask any point with bad data'''\n",
    "dontBotherMask = np.zeros_like(data0).flatten()\n",
    "\n",
    "# produce an index for transforming data to list of (overlapping) testGrids\n",
    "transformSubI = np.arange(-int(np.floor(np.sqrt(testGridSize)/2)),int(np.floor(np.sqrt(testGridSize)/2))+1,1)\n",
    "transformSubI = np.tile(transformSubI.reshape(transformSubI.size, 1),(1, int(np.sqrt(testGridSize))))\n",
    "transformSubI = transformSubI.reshape((1, transformSubI.size))\n",
    "#print transformSubI\n",
    "\n",
    "# produce an index for transforming data to list of (overlapping) testGrids\n",
    "transformSubJ = np.arange(-int(np.floor(np.sqrt(testGridSize)/2)),int(np.floor(np.sqrt(testGridSize)/2))+1,1)\n",
    "transformSubJ = np.tile(transformSubJ.reshape(1, transformSubJ.size),(int(np.sqrt(testGridSize)), 1))\n",
    "transformSubJ = transformSubJ.reshape((1, transformSubJ.size))\n",
    "#print transformSubJ\n",
    "\n",
    "# broadcast the transformSubs with their big counterparts\n",
    "transformSubI = transformSubI + I\n",
    "transformSubJ = transformSubJ + J\n",
    "\n",
    "#now remove the test grids if they:\n",
    "#    1) contain any negative numbers in I or J\n",
    "#    2) if the dontBotherMask is set\n",
    "removeData = np.logical_or(np.logical_or(transformSubI<0,transformSubJ<0),\n",
    "                           np.logical_or(transformSubI>=data1.shape[0],transformSubJ>=data1.shape[1]))\n",
    "removeData = np.logical_or(np.any(removeData,axis=1),dontBotherMask)\n",
    "\n",
    "transformSubI = transformSubI[removeData==0,:]\n",
    "transformSubJ = transformSubJ[removeData==0,:]\n",
    "locationData = locationData[removeData==0,:]\n",
    "N = np.arange(0,locationData.shape[0])\n",
    "N = N.reshape((N.size, 1))\n",
    "\n",
    "locationData = np.hstack((N,locationData))\n",
    "\n",
    "# transform the data:\n",
    "data0 = data0[transformSubI,transformSubJ]\n",
    "data1 = data1[transformSubI,transformSubJ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(31, [13, 14, 15, 21, 22, 23, 29, 30, 37, 38, 39, 45, 46, 47])]\n"
     ]
    }
   ],
   "source": [
    "distThreshold = 30\n",
    "\n",
    "#define a function which returns the Euclidean distance between two points (in the x, y plane)\n",
    "def distance_calc(x1,y1,x2,y2):\n",
    "    \n",
    "    #keepShape = x1.size\n",
    "    #distance = np.sqrt((np.tile(x1.reshape((x1.size, 1)), (1, keepShape)) - \n",
    "    #                      np.tile(x2.reshape((1, x2.size)), (keepShape, 1)))**2 + \n",
    "    #                    (np.tile(y1.reshape((y1.size, 1)), (1, keepShape)) - \n",
    "    #                      np.tile(y2.reshape((1, y2.size)), (keepShape, 1)))**2)\n",
    "    \n",
    "    distance = np.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "    \n",
    "    return distance\n",
    "\n",
    "#reformat the distanceDictRDD to the correct dictionary-like object\n",
    "def makeDistFunc(line):\n",
    "    \n",
    "    key = line[0]\n",
    "    tempList = line[1]\n",
    "    \n",
    "    return (key, list(line[1]))\n",
    "\n",
    "# make an RDD of the index and location data, and also a broadcast variable\n",
    "locationBroadcast = sc.broadcast(locationData)\n",
    "locationDataRDD = sc.parallelize(locationData, 4)\n",
    "\n",
    "# take the cartesian product and compute the distance between each point: filter by distance threshold\n",
    "# distanceDataRDD contains the info about each point, and the other points which are less than the distance\n",
    "# threshold away\n",
    "'''this process needs to be sped up'''\n",
    "distanceDataRDD = (locationDataRDD\n",
    "                   .cartesian(locationDataRDD)\n",
    "                   .map(lambda (a, b): (a, (b, distance_calc(a[3],a[4],b[3],b[4]))))\n",
    "                   .filter(lambda (a, (b, dist)): dist < distThreshold and dist != 0)\n",
    "                   .map(lambda (a, (b, dist)): (a[0], b[0])))\n",
    "\n",
    "# now reduce by key to list those points within range\n",
    "distanceDictRDD = (distanceDataRDD\n",
    "                   .groupByKey()\n",
    "                   .map(makeDistFunc)\n",
    "                   .sortByKey()\n",
    "                   .cache())\n",
    "\n",
    "print distanceDictRDD.filter(lambda (a, b): a==31).collect()\n",
    "#print np.arange(0,100).reshape((10,10))\n",
    "#print distanceDictRDD.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([4, 8]), array([-20,   0]))\n"
     ]
    }
   ],
   "source": [
    "# define a function which collects the broadcast data into an RDD\n",
    "# can also put correlation in here if you like\n",
    "def computeCorrelation(line):\n",
    "        \n",
    "    # extract relevant data from the daily broadcast variables\n",
    "    getdata0 = data0Broadcast.value[line[0],:]\n",
    "    getdata1 = data1Broadcast.value[line[1],:]\n",
    "        \n",
    "    # tile data0 for fast correlation\n",
    "    getdata0 = np.tile(getdata0-np.mean(getdata0),(getdata1.shape[0], 1))\n",
    "    getdata1 = getdata1 - np.mean(getdata1,axis=1).reshape((getdata1.shape[0], 1))\n",
    "    \n",
    "    correl = np.sum(getdata0*getdata1,axis=1) / (np.sqrt(np.sum(getdata0*getdata0,axis=1)) * np.sqrt(np.sum(getdata1*getdata1,axis=1)))\n",
    "    \n",
    "    return (line[0], line[1], correl)\n",
    "\n",
    "# find the largest correlation\n",
    "'''this also needs work on: threshold value, and sharpness of peak.\n",
    "Can find threshold value by computing distribution of correlations amongst motions which could never occur.\n",
    "Then take 90th percentile: should remove all but 10% of bad matches.'''\n",
    "def findLargestCorrel(line):\n",
    "    \n",
    "    correl = line[2]\n",
    "    maximum = line[1][correl.argmax()]\n",
    "\n",
    "    xvel = locationBroadcast.value[maximum,4] - locationBroadcast.value[line[0],4]\n",
    "    yvel = locationBroadcast.value[maximum,3] - locationBroadcast.value[line[0],3]\n",
    "    \n",
    "    return (np.array([locationBroadcast.value[line[0],1], locationBroadcast.value[line[0],2]]), np.array([yvel, xvel]))\n",
    "\n",
    "print findLargestCorrel(computeCorrelation((31, [13, 14, 15, 21, 22, 23, 29, 30, 37, 38, 39, 45, 46, 47])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turn data0 and data1 into broadcast variables\n",
    "data0Broadcast = sc.broadcast(data0)\n",
    "data1Broadcast = sc.broadcast(data1)\n",
    "\n",
    "correlationRDD = (distanceDictRDD\n",
    "            .map(computeCorrelation)\n",
    "            .map(findLargestCorrel)\n",
    "            .cache())\n",
    "            \n",
    "#print correlationRDD.collect()\n",
    "#print np.arange(0,100).reshape((10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      " [ nan  10.   0.   0.   0.   0.   0.   0.  10.  nan]\n",
      " [ nan  10.   0.   0.   0.   0.   0.   0. -10.  nan]\n",
      " [ nan   0.   0.   0.   0.   0.   0.   0.  10.  nan]\n",
      " [ nan  20.   0.   0.   0.   0.   0.   0.  20.  nan]\n",
      " [ nan -10.   0.   0.   0.   0.   0.   0.   0.  nan]\n",
      " [ nan -10.   0.   0.   0.   0.   0.   0.  20.  nan]\n",
      " [ nan -10.   0.   0.   0.   0.   0.   0.  10.  nan]\n",
      " [ nan   0.   0.   0.   0.   0.   0.   0. -10.  nan]\n",
      " [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]]\n",
      "[[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      " [ nan  20. -10. -10. -10.  10.  10.  10. -20.  nan]\n",
      " [ nan   0. -10. -10. -10.  10.  10.  10. -20.  nan]\n",
      " [ nan  10. -10. -10. -10. -10.  10.  10. -20.  nan]\n",
      " [ nan   0. -10. -10. -10.  10.  10.  10. -20.  nan]\n",
      " [ nan  20. -10. -10. -10.  10.  10.  10. -20.  nan]\n",
      " [ nan  20. -10. -10. -10.  10.  10.  10.   0.  nan]\n",
      " [ nan  20. -10. -10. -10. -10.  10.  10. -20.  nan]\n",
      " [ nan  20. -10. -10. -10. -10.  10.  10. -10.  nan]\n",
      " [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]]\n"
     ]
    }
   ],
   "source": [
    "# now need to reconstruct original data format\n",
    "# to do this, we are going to collect 2 RDDS\n",
    "\n",
    "# one for the location of output\n",
    "outputLocation = np.array(correlationRDD\n",
    "                 .flatMap(lambda (a, b): a)\n",
    "                 .collect())\n",
    "outputLocation = outputLocation.reshape((outputLocation.size/2, 2))\n",
    "\n",
    "# one for the distance travelled\n",
    "outputDist = np.array(correlationRDD\n",
    "                 .flatMap(lambda (a, b): b)\n",
    "                 .collect())\n",
    "outputDist = outputDist.reshape((outputDist.size/2, 2))\n",
    "\n",
    "\n",
    "yDist = np.ones((np.sqrt(gridSize),np.sqrt(gridSize)))*np.nan\n",
    "xDist = np.ones((np.sqrt(gridSize),np.sqrt(gridSize)))*np.nan\n",
    "\n",
    "'''These need to be divided by the time period'''\n",
    "yDist[outputLocation[:,0],outputLocation[:,1]] = outputDist[:,0]\n",
    "xDist[outputLocation[:,0],outputLocation[:,1]] = outputDist[:,1]\n",
    "\n",
    "#print outputLocation\n",
    "#print outputLocation[1:2:]\n",
    "print yDist\n",
    "print xDist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
	# add a comment here

   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
